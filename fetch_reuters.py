#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·¯é€ç¤¾æ–°é—»è·å–å·¥å…·
ç‹¬ç«‹è„šæœ¬ï¼Œå¯ä»¥æ”¾åœ¨ä»»ä½•æ–‡ä»¶å¤¹è¿è¡Œ
åªéœ€è¦å®‰è£… requests åº“: pip install requests

æ³¨æ„ï¼šReuters API å¯èƒ½éœ€è¦ç§‘å­¦ä¸Šç½‘æ‰èƒ½è®¿é—®
"""

import requests
from datetime import datetime
import json


HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/json',
    'Accept-Language': 'en-US,en;q=0.9',
}


def get_wire_news():
    """è·å–Reuters Wireæ–°é—»"""
    url = "https://www.reuters.com/pf/api/v3/content/fetch/articles-by-section-alias-or-id-v1"
    query = {
        "arc-site": "reuters",
        "called_from_a_]component": True,
        "fetch_type": "section",
        "offset": 0,
        "section_id": "/wire/",
        "size": 30,
        "website": "reuters"
    }
    params = {
        "query": json.dumps(query),
        "_website": "reuters"
    }
    
    try:
        response = requests.get(url, params=params, headers=HEADERS, timeout=15)
        response.raise_for_status()
        data = response.json()
        
        news_list = []
        for item in data.get("result", {}).get("articles", []):
            # è§£ææ—¶é—´
            time_str = ""
            published = item.get("published_time", "")
            if published:
                try:
                    dt = datetime.fromisoformat(published.replace('Z', '+00:00'))
                    time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    time_str = published
            
            news = {
                "id": item.get("id"),
                "title": item.get("title", ""),
                "url": f"https://www.reuters.com{item.get('canonical_url', '')}",
                "time": time_str,
                "summary": item.get("description", "")[:150] if item.get("description") else ""
            }
            news_list.append(news)
        
        return news_list
    except Exception as e:
        print(f"è·å–Wireæ–°é—»å¤±è´¥: {e}")
        return []


def get_business_news():
    """è·å–Reuterså•†ä¸šæ–°é—»"""
    url = "https://www.reuters.com/pf/api/v3/content/fetch/articles-by-section-alias-or-id-v1"
    query = {
        "arc-site": "reuters",
        "called_from_a_component": True,
        "fetch_type": "section",
        "offset": 0,
        "section_id": "/business/",
        "size": 30,
        "website": "reuters"
    }
    params = {
        "query": json.dumps(query),
        "_website": "reuters"
    }
    
    try:
        response = requests.get(url, params=params, headers=HEADERS, timeout=15)
        response.raise_for_status()
        data = response.json()
        
        news_list = []
        for item in data.get("result", {}).get("articles", []):
            # è§£ææ—¶é—´
            time_str = ""
            published = item.get("published_time", "")
            if published:
                try:
                    dt = datetime.fromisoformat(published.replace('Z', '+00:00'))
                    time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    time_str = published
            
            news = {
                "id": item.get("id"),
                "title": item.get("title", ""),
                "url": f"https://www.reuters.com{item.get('canonical_url', '')}",
                "time": time_str,
                "summary": item.get("description", "")[:150] if item.get("description") else ""
            }
            news_list.append(news)
        
        return news_list
    except Exception as e:
        print(f"è·å–å•†ä¸šæ–°é—»å¤±è´¥: {e}")
        return []


def get_markets_news():
    """è·å–Reuterså¸‚åœºæ–°é—»"""
    url = "https://www.reuters.com/pf/api/v3/content/fetch/articles-by-section-alias-or-id-v1"
    query = {
        "arc-site": "reuters",
        "called_from_a_component": True,
        "fetch_type": "section",
        "offset": 0,
        "section_id": "/markets/",
        "size": 30,
        "website": "reuters"
    }
    params = {
        "query": json.dumps(query),
        "_website": "reuters"
    }
    
    try:
        response = requests.get(url, params=params, headers=HEADERS, timeout=15)
        response.raise_for_status()
        data = response.json()
        
        news_list = []
        for item in data.get("result", {}).get("articles", []):
            # è§£ææ—¶é—´
            time_str = ""
            published = item.get("published_time", "")
            if published:
                try:
                    dt = datetime.fromisoformat(published.replace('Z', '+00:00'))
                    time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    time_str = published
            
            news = {
                "id": item.get("id"),
                "title": item.get("title", ""),
                "url": f"https://www.reuters.com{item.get('canonical_url', '')}",
                "time": time_str,
                "summary": item.get("description", "")[:150] if item.get("description") else ""
            }
            news_list.append(news)
        
        return news_list
    except Exception as e:
        print(f"è·å–å¸‚åœºæ–°é—»å¤±è´¥: {e}")
        return []


def get_reuters_rss():
    """é€šè¿‡RSSè·å–Reutersæ–°é—»ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    # Reuters RSS feeds
    feeds = [
        ("å•†ä¸šæ–°é—»", "https://www.reutersagency.com/feed/?best-topics=business-finance&post_type=best"),
        ("ä¸–ç•Œæ–°é—»", "https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best"),
    ]
    
    all_news = []
    
    for feed_name, url in feeds:
        try:
            response = requests.get(url, headers=HEADERS, timeout=15)
            response.raise_for_status()
            
            # ç®€å•è§£æRSS XML
            import re
            items = re.findall(r'<item>(.*?)</item>', response.text, re.DOTALL)
            
            for item in items[:10]:
                title = re.search(r'<title><!\[CDATA\[(.*?)\]\]></title>', item) or re.search(r'<title>(.*?)</title>', item)
                link = re.search(r'<link>(.*?)</link>', item)
                pubDate = re.search(r'<pubDate>(.*?)</pubDate>', item)
                description = re.search(r'<description><!\[CDATA\[(.*?)\]\]></description>', item, re.DOTALL) or \
                              re.search(r'<description>(.*?)</description>', item, re.DOTALL)
                
                news = {
                    "title": title.group(1) if title else "",
                    "url": link.group(1) if link else "",
                    "time": pubDate.group(1) if pubDate else "",
                    "summary": description.group(1)[:150] if description else "",
                    "category": feed_name
                }
                if news["title"]:
                    all_news.append(news)
        except Exception as e:
            print(f"è·å–{feed_name} RSSå¤±è´¥: {e}")
    
    return all_news


def print_news(news_list, title):
    """æ ¼å¼åŒ–æ‰“å°æ–°é—»åˆ—è¡¨"""
    print("\n" + "=" * 60)
    print(f"ğŸ“° {title}")
    print("=" * 60)
    
    if not news_list:
        print("æš‚æ— æ•°æ® (å¯èƒ½éœ€è¦ç§‘å­¦ä¸Šç½‘)")
        return
    
    for i, news in enumerate(news_list, 1):
        print(f"\n{i}. {news['title']}")
        if news.get('time'):
            print(f"   ğŸ• {news['time']}")
        if news.get('url'):
            print(f"   ğŸ”— {news['url']}")
        if news.get('summary'):
            # æ¸…ç†HTMLæ ‡ç­¾
            import re
            summary = re.sub(r'<[^>]+>', '', news['summary'])
            print(f"   ğŸ“ {summary}...")


def main():
    print("\nğŸŒ Reutersï¼ˆè·¯é€ç¤¾ï¼‰æ–°é—»è·å–å·¥å…·")
    print("âš ï¸  æ³¨æ„ï¼šå¯èƒ½éœ€è¦ç§‘å­¦ä¸Šç½‘æ‰èƒ½æ­£å¸¸è®¿é—®")
    print("æ­£åœ¨è·å–æœ€æ–°æ•°æ®...\n")
    
    # 1. å°è¯•è·å–RSSæ–°é—»ï¼ˆæ›´ç¨³å®šï¼‰
    rss_news = get_reuters_rss()
    print_news(rss_news[:10], "Reuters RSSæ–°é—» (æœ€æ–°10æ¡)")
    
    # 2. è·å–Wireæ–°é—»
    wire = get_wire_news()
    print_news(wire[:10], "å®æ—¶æ–°é—» (æœ€æ–°10æ¡)")
    
    # 3. è·å–å•†ä¸šæ–°é—»
    business = get_business_news()
    print_news(business[:10], "å•†ä¸šæ–°é—» (æœ€æ–°10æ¡)")
    
    # 4. è·å–å¸‚åœºæ–°é—»
    markets = get_markets_news()
    print_news(markets[:10], "å¸‚åœºæ–°é—» (æœ€æ–°10æ¡)")
    
    print("\n" + "=" * 60)
    print("âœ… è·å–å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
